{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AML_HW10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "vUTgGUi4JEft"
      },
      "cell_type": "markdown",
      "source": [
        "# Homework 10 - CIFAR10 Image Classification with PyTorch"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "T5Kx8gJBJEW_"
      },
      "cell_type": "markdown",
      "source": [
        "## About"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "aRAalfd1GITX"
      },
      "cell_type": "markdown",
      "source": [
        "The goal of the homework is to train a convolutional neural network on the standard CIFAR10 image classfication dataset.\n",
        "\n",
        "When solving machine learning tasks using neural networks, one typically starts with a simple network architecture and then improves the network by adding new layers, retraining, adjusting parameters, retraining, etc.  We attempt to illustrate this process below with several architecture improvements.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OCl5gakxJes-"
      },
      "cell_type": "markdown",
      "source": [
        "## Dev Environment\n",
        "### Working on Google Colab\n",
        "You may choose to work locally or on Google Colaboratory. You have access to free compute through this service.   Colab is recommended since it will be setup correctly and will have access to GPU resources.\n",
        "1. Visit https://colab.research.google.com/drive \n",
        "2. Navigate to the **`Upload`** tab, and upload your `HW10.ipynb`\n",
        "3. Now on the top right corner, under the `Comment` and `Share` options, you should see a `Connect` option. Once you are connected, you will have access to a VM with 12GB RAM, 50 GB disk space and a single GPU. The dropdown menu will allow you to connect to a local runtime as well.\n",
        "\n",
        "**Notes:** \n",
        "* **If you do not have a working setup for Python 3, this is your best bet. It will also save you from heavy installations like `tensorflow` if you don't want to deal with those.**\n",
        "* ***There is a downside*. You can only use this instance for a single 12-hour stretch, after which your data will be deleted, and you would have redownload all your datasets, any libraries not already on the VM, and regenerate your logs**.\n",
        "\n",
        "\n",
        "### Installing PyTorch and Dependencies\n",
        "\n",
        "The instructions for installing and setting up PyTorch can be found at https://pytorch.org/get-started/locally/. Make sure you follow the instructions for your machine. For any of the remaining libraries used in this assignment:\n",
        "* We have provided a `hw8_requirements.txt` file on the homework web page. \n",
        "* Download this file, and in the same directory you can run `pip3 install -r hw8_requirements.txt`\n",
        "â€‹\n",
        "Check that PyTorch installed correctly by running the following:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IaJ6BLrAJeCU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.rand(5, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nEmmCk1IJiGn"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 0 Imports and Basic Setup  (5 Points)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Y3mTddKuXPBU"
      },
      "cell_type": "markdown",
      "source": [
        "First, import the required libraries as follows. The libraries we will use will be the same as those in HW8. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UwtDsq3VbrNY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QWKp_UGlWTyR"
      },
      "cell_type": "markdown",
      "source": [
        "**GPU Support**\n",
        "\n",
        "Training of large network can take a long time. PyTorch supports GPU with just a small amount of effort.\n",
        "\n",
        "When creating our networks, we will call \n",
        "`net.to(device)` to tell the network to train on the GPU, if one is available.  Note, if the network utilizes the GPU, it is important that any tensors we use with it (such as the data) also reside on the CPU.  Thus, a call like `images = images.to(device)` is necessary with any data we want to use with the GPU.\n",
        "\n",
        "Note: If you can't get access to a GPU, don't worry to much.  Since we use very small networks, the difference between CPU and GPU isn't large and in some cases GPU will actually be slower."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gbGGmnIXYca9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.cuda as cuda\n",
        "\n",
        "# Use a GPU, i.e. cuda:0 device if it available.\n",
        "device = torch.device(\"cuda:0\" if cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "07RjQbhF1tiK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training Code"
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "_DZm8ammPCbL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "  \"\"\"NN Module that flattens the incoming tensor.\"\"\"\n",
        "  def forward(self, input):\n",
        "    return input.view(input.size(0), -1)\n",
        "  \n",
        "def train(model, train_loader, test_loader, loss_func, opt, num_epochs=10):\n",
        "  all_training_loss = np.zeros((0,2))\n",
        "  all_training_acc = np.zeros((0,2))\n",
        "  all_test_loss = np.zeros((0,2))\n",
        "  all_test_acc = np.zeros((0,2))\n",
        "  \n",
        "  training_step = 0\n",
        "  training_loss, training_acc = 2.0, 0.0\n",
        "  print_every = 1000\n",
        "  \n",
        "  start = time.clock()\n",
        "  \n",
        "  for i in range(num_epochs):\n",
        "    epoch_start = time.clock() \n",
        "   \n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      opt.zero_grad()\n",
        "\n",
        "      preds = model(images)\n",
        "      loss = loss_func(preds, labels)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      \n",
        "      training_loss += loss.item()\n",
        "      training_acc += (torch.argmax(preds, dim=1)==labels).float().mean()\n",
        "      \n",
        "      if training_step % print_every == 0:\n",
        "        training_loss /= print_every\n",
        "        training_acc /= print_every\n",
        "        \n",
        "        all_training_loss = np.concatenate((all_training_loss, [[training_step, training_loss]]))\n",
        "        all_training_acc = np.concatenate((all_training_acc, [[training_step, training_acc]]))\n",
        "        \n",
        "        print('  Epoch %d @ step %d: Train Loss: %3f, Train Accuracy: %3f' % (\n",
        "            i, training_step, training_loss, training_acc))\n",
        "        training_loss, training_acc = 0.0, 0.0\n",
        "        \n",
        "      training_step+=1\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      validation_loss, validation_acc = 0.0, 0.0\n",
        "      count = 0\n",
        "      for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = model(images)\n",
        "        validation_loss+=loss_func(output,labels)\n",
        "        validation_acc+=(torch.argmax(output, dim=1) == labels).float().mean()\n",
        "        count += 1\n",
        "      validation_loss/=count\n",
        "      validation_acc/=count\n",
        "      \n",
        "      all_test_loss = np.concatenate((all_test_loss, [[training_step, validation_loss]]))\n",
        "      all_test_acc = np.concatenate((all_test_acc, [[training_step, validation_acc]]))\n",
        "      \n",
        "      epoch_time = time.clock() - epoch_start\n",
        "      \n",
        "      print('Epoch %d Test Loss: %3f, Test Accuracy: %3f, time: %.1fs' % (\n",
        "          i, validation_loss, validation_acc, epoch_time))\n",
        "      \n",
        "  total_time = time.clock() - start\n",
        "  print('Final Test Loss: %3f, Test Accuracy: %3f, Total time: %.1fs' % (\n",
        "      validation_loss, validation_acc, total_time))\n",
        "\n",
        "  return {'loss': { 'train': all_training_loss, 'test': all_test_loss },\n",
        "          'accuracy': { 'train': all_training_acc, 'test': all_test_acc }}\n",
        "\n",
        "def plot_graphs(model_name, metrics):\n",
        "  for metric, values in metrics.items():\n",
        "    for name, v in values.items():\n",
        "      plt.plot(v[:,0], v[:,1], label=name)\n",
        "    plt.title(f'{metric} for {model_name}')\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Training Steps\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1RupXQbCaXb3"
      },
      "cell_type": "markdown",
      "source": [
        "Load the** CIFA-10** dataset and define the transformations. You may also want to print its structure, size, as well as sample a few images to get a sense of how to design the network. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mIJmLIgaZ_d3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir hw10_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aDBbPmPPaQuG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download the data.\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transformations = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_set = datasets.CIFAR10(root='hw10_data/', download=True, transform=transformations)\n",
        "test_set = datasets.CIFAR10(root='hw10_data', download=True, train=False, transform=transformations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dfomsGyJiKz9"
      },
      "cell_type": "markdown",
      "source": [
        "Use `DataLoader` to create a loader for the training set and a loader for the testing set. You can use a `batch_size` of 8 to start, and change it if you wish."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ioe04mbSiQiV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 8\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "input_shape = np.array(train_set[0][0]).shape\n",
        "input_dim = input_shape[1]*input_shape[2]*input_shape[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fGoPdmuUOOiE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3T2-qkh8frqF"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1 CIFAR10 with Fully Connected Neural Netowrk (25 Points)\n",
        "\n",
        "As a warm-up, let's begin by training a two-layer fully connected neural network model on ** CIFAR-10** dataset. You may go back to check HW8 for some basics.\n",
        "\n",
        "We will give you this code to use as a baseline to compare against your CNN models."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ck1CGpRycRFE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TwoLayerModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TwoLayerModel, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      Flatten(), \n",
        "      nn.Linear(input_dim, 64), \n",
        "      nn.ReLU(), \n",
        "      nn.Linear(64, 10))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "model = TwoLayerModel().to(device)\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "# Training epoch should be about 15-20 sec each on GPU.\n",
        "metrics = train(model, train_loader, test_loader, loss, optimizer, training_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qjbncuMnKpf7"
      },
      "cell_type": "markdown",
      "source": [
        "**Plot the model results**\n",
        "\n",
        "Normally we would want to use Tensorboard for looking at metrics.  However, if colab reset while we are working, we might lose our logs and therefore our metrics.  Let's just plot some graphs that will survive across colab instances."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CuVL9MJ_D92h",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_graphs(\"TwoLayerModel\", metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4_xi8bkkfsM7"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2 Convolutional Neural Network (CNN) (35 Points)\n",
        "\n",
        "Now, let's design a convolution neural netwrok!\n",
        "\n",
        "Build a simple CNN model, inserting 2 CNN layers in from of our 2 layer fully connect model from above:\n",
        "\n",
        "1. A convolution with\t3x3 filter, 16 output channels, stride = 1, padding=1\n",
        "2. A ReLU activation\n",
        "2. A Max-Pooling layer with 2x2 window\n",
        "3. A convolution,\t3x3 filter, 16 output channels, stride = 1, padding=1\n",
        "4. A ReLU activation\n",
        "4. Flatten layer\n",
        "5. Fully connected linear layer with output size 64\n",
        "6. ReLU\n",
        "7. Fully connected linear layer, with output size 10\n",
        "\n",
        "You will have to figure out the input sizes of the first fully connnected layer based on the previous layer sizes. Note that you also need to fill those in the report section (see report section in the notebook for details) "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tLXjjywngO7g",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "class ConvModel(nn.Module):\n",
        "  # Your Code Here\n",
        "    def __init__(self):\n",
        "        super(ConvModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 16, 3, 1, 1)\n",
        "        self.fc1 = nn.Linear(16 * 8 * 8, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "  \n",
        "model = ConvModel().to(device)\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "metrics = train(model, train_loader, test_loader, loss, optimizer, training_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8BGTJw-TPTfh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_graphs(\"ConvModel\", metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oFAc6IORmzCJ"
      },
      "cell_type": "markdown",
      "source": [
        "Do you notice the improvement over the accuracy compared to that in Part 1?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hXpJgsjhftDP"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3 Open Design Competition (35 Points + 10 bonus points)\n",
        "\n",
        "Try to beat the previous models by adding additional layers, changing parameters, etc.  You should add at least one layer.\n",
        "\n",
        "Possible changes include:\n",
        "*   Dropout\n",
        "*   Batch Normalization\n",
        "*   More layers\n",
        "*   Residual Connections (harder)\n",
        "*   Change layer size\n",
        "*   Pooling layers, stride\n",
        "*   Different optimizer\n",
        "*   Train for longer\n",
        "\n",
        "Once you have a model you think is great, evaluate it against our hidden test data (see hidden_loader above) and upload the results to the leader board on gradescope.  **The top 3 scorers will get a bonus 10 points.**\n",
        "\n",
        "You can steal model structures found on the internet if you want.  The only constraint is that **you must train the model from scratch**.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QZFkuraiZOaT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# You Awesome Super Best model code here\n",
        "import torch.nn.functional as F\n",
        "class ConvModelPart3(nn.Module):\n",
        "  # Your Code Here\n",
        "    def __init__(self):\n",
        "        super(ConvModelPart3, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 16, 3, 1, 1)\n",
        "        self.conv3 = nn.Conv2d(16, 64, 3, 1, 1)\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 4 * 4)\n",
        "        # x = self.drop_out(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "  \n",
        "model = ConvModelPart3().to(device)\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "metrics = train(model, train_loader, test_loader, loss, optimizer, training_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xbx5REUdc-Xs"
      },
      "cell_type": "markdown",
      "source": [
        "**What changes did you make to improve your model?**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uItu0w4fZTuG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_graphs(\"AwesomeModel\", metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rWlP2WsodgK0"
      },
      "cell_type": "markdown",
      "source": [
        "After you get a nice model, download the test_file.zip and unzip it to get test_file.pt. In colab, you can explore your files from the left side bar. You can also download the files to your machine from there."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Z6UZInUCdfQ1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://courses.engr.illinois.edu/cs498aml/sp2019/homeworks/test_file.zip\n",
        "unzip test_file.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JQaDEpp93MZS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7g5caFPreMLT"
      },
      "cell_type": "markdown",
      "source": [
        "Then use your model to predict the label of the test images. Fill the remaining code below, where x has two dimensions (batch_size x one image size). Remember to reshpe x accordingly before feeding it into your model. The submission.txt should contain one predicted label (0~9) each line. Submit your submission.txt to the competition in gradscope."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yNdYAH9XeLlb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.utils.data as Data\n",
        "import numpy as np\n",
        "\n",
        "test_file = 'test_file.pt'\n",
        "pred_file = 'submission.txt'\n",
        "\n",
        "f_pred = open(pred_file,'w') \n",
        "tensor = torch.load(test_file)\n",
        "torch_dataset = Data.TensorDataset(tensor)  \n",
        "test_loader = torch.utils.data.DataLoader(torch_dataset, batch_size, shuffle=False, num_workers=2)\n",
        "      \n",
        "for ele in test_loader:\n",
        "    x = ele[0]\n",
        "    \n",
        "    # Fill your code here\n",
        "    \n",
        "    classes = range(10)\n",
        "\n",
        "    x = x.reshape(-1,3,32,32)\n",
        "    x = x.to(device)\n",
        "    output = model(x)\n",
        "    \n",
        "    prediction = np.sum(torch.Tensor.numpy(output.cpu().data), axis=0)\n",
        "    index = np.argmax(prediction)\n",
        "    f_pred.write(str(index))\n",
        "  \n",
        "    #\n",
        "    f_pred.write('\\n')\n",
        "    \n",
        "f_pred.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8-L6F0CMqc-A"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Report\n",
        "\n",
        "## Part 0: Imports and Basic Setup (5 Points)\n",
        "Nothing to report for this part. You will be just scored for finishing the setup.\n",
        "\n",
        "## Part 1: Fully connected neural networks (25 Points)\n",
        "\n",
        "Test (on validation set) accuracy (5 Points): 39.07%\n",
        "\n",
        "Test loss (5 Points): 1.70355\n",
        "\n",
        "Training time (5 Points): 82.9s\n",
        "\n",
        "\n",
        "Plots:\n",
        "\n",
        "- Plot a graph of accuracy on validation set vs training steps (5 Points)\n",
        "\n",
        "- Plot a graph of loss on validation set vs training steps (5 Points)\n",
        "\n",
        "\n",
        "## Part 2: Convolution Network (Basic) (35 Points)\n",
        "\n",
        "Tensor dimensions: A good way to debug your network for size mismatches is to print the dimension of output after every layers:\n",
        "\n",
        "(10 Points)\n",
        "\n",
        "Output dimension after 1st conv layer: 16,32,32\n",
        "\n",
        "Output dimension after 1st max pooling: 16,16,16\n",
        "\n",
        "Output dimension after 2nd conv layer: 16,16,16\n",
        "\n",
        "Output dimension after flatten layer: 16,8,8\n",
        "\n",
        "Output dimension after 1st fully connected layer: 1,1024\n",
        "\n",
        "Output dimension after 2nd fully connected layer: 1,10\n",
        "\n",
        "\n",
        "Test (on validation set) Accuracy (5 Points): 58.86%\n",
        "\n",
        "Test loss (5 Points): 1.170402\n",
        "\n",
        "Training time (5 Points): 121.1s\n",
        "\n",
        "\n",
        "Plots:\n",
        "\n",
        "- Plot a graph of accuracy on validation set vs training steps (5 Points)\n",
        "\n",
        "- Plot a graph of loss on validation set vs training steps (5 Points)\n",
        "\n",
        "\n",
        "\n",
        "## Part 3: Convolution Network (Add one or more  suggested changes) (35 Points)\n",
        "\n",
        "Describe the additional changes implemented, your intuition for as to why it works, you may also describe other approaches you experimented with (10 Points):\n",
        "\n",
        "Additional layer is added to the part 2 implementation, which helps to extract more features and thus increase accuracy. \n",
        "I have also added dropout and Batch Normalization, which decreased the accuracy. I have not included them into the implementation.\n",
        "\n",
        "\n",
        "Test (on validation set) Accuracy (5 Points): 61.51%\n",
        "\n",
        "Test loss (5 Points): 1.102304\n",
        "\n",
        "Training time (5 Points): 144.0s\n",
        "\n",
        "\n",
        "Plots:\n",
        "\n",
        "- Plot a graph of accuracy on validation set vs training steps (5 Points)\n",
        "\n",
        "- Plot a graph of loss on validation set vs training steps (5 Points)\n",
        "\n",
        "10 bonus points will be awarded to top 3 scorers on leaderboard (in case of tie for 3rd position everyone tied for 3rd position will get the bonus)"
      ]
    },
    {
      "metadata": {
        "id": "cMpXJT3p1tjB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}